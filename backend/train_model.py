"""
train_model.py
åˆ† 11 è¼ªè¨“ç·´ï¼Œæ¯è¼ª 5 å€‹ epochï¼Œæ”¯æ´æ–·é»žçºŒè¨“
æŽ¡ç”¨ MobileNetV2 é·ç§»å­¸ç¿’ï¼Œç¬¬ 6 è¼ªèµ·é€²è¡Œ Fine-tuning
"""
import os
import sys
import json
import time
import numpy as np
from pathlib import Path

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"  # æ¸›å°‘ TF æ—¥èªŒ

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# â”€â”€â”€ è·¯å¾‘èˆ‡è¶…åƒæ•¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR       = Path(__file__).parent
TRAIN_DIR      = BASE_DIR / "data" / "train"
VAL_DIR        = BASE_DIR / "data" / "val"
MODEL_DIR      = BASE_DIR / "models"
CKPT_DIR       = BASE_DIR / "checkpoints"
CLASS_JSON     = BASE_DIR / "data" / "class_names.json"
PROGRESS_FILE  = CKPT_DIR / "progress.json"

IMG_SIZE       = (224, 224)
BATCH_SIZE     = 32
TOTAL_ROUNDS   = 11
EPOCHS_PER_ROUND = 5
LR_INITIAL     = 1e-3
LR_FINETUNE    = 1e-4
FINETUNE_START = 6       # ç¬¬å¹¾è¼ªé–‹å§‹è§£å‡ base model
UNFREEZE_LAYERS = 30     # è§£å‡æœ€å¾Œå¹¾å±¤

MODEL_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR.mkdir(parents=True, exist_ok=True)

# â”€â”€â”€ é€²åº¦ç®¡ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_progress():
    if PROGRESS_FILE.exists():
        with open(PROGRESS_FILE) as f:
            return json.load(f)
    return {"completed_rounds": 0, "history": [], "best_val_acc": 0.0}

def save_progress(prog):
    with open(PROGRESS_FILE, "w") as f:
        json.dump(prog, f, indent=2)

# â”€â”€â”€ è³‡æ–™é›† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_datasets():
    if not TRAIN_DIR.exists():
        print("âŒ æ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™ï¼Œè«‹å…ˆåŸ·è¡Œ python download_dataset.py")
        sys.exit(1)

    augmentation = keras.Sequential([
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.2),
        layers.RandomZoom(0.15),
        layers.RandomContrast(0.1),
        layers.RandomBrightness(0.1),
    ], name="augmentation")

    AUTOTUNE = tf.data.AUTOTUNE

    def preprocess_train(x, y):
        x = augmentation(x, training=True)
        return tf.cast(x, tf.float32) / 255.0, y

    def preprocess_val(x, y):
        return tf.cast(x, tf.float32) / 255.0, y

    train_ds = tf.keras.utils.image_dataset_from_directory(
        TRAIN_DIR,
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        label_mode="categorical",
        shuffle=True,
        seed=42,
    ).map(preprocess_train, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)

    val_ds = tf.keras.utils.image_dataset_from_directory(
        VAL_DIR,
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        label_mode="categorical",
        shuffle=False,
    ).map(preprocess_val, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)

    # å–å¾—é¡žåˆ¥åç¨±
    raw_ds = tf.keras.utils.image_dataset_from_directory(TRAIN_DIR, batch_size=1)
    class_names = raw_ds.class_names
    num_classes = len(class_names)

    # å„²å­˜é¡žåˆ¥åç¨±
    with open(CLASS_JSON, "w", encoding="utf-8") as f:
        json.dump({"classes": class_names, "num_classes": num_classes}, f, ensure_ascii=False, indent=2)

    print(f"âœ… è³‡æ–™é›†è¼‰å…¥å®Œæˆï¼š{num_classes} å€‹é¡žåˆ¥")
    return train_ds, val_ds, class_names, num_classes

# â”€â”€â”€ æ¨¡åž‹å»ºæ§‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_model(num_classes, lr=LR_INITIAL):
    base = keras.applications.MobileNetV2(
        input_shape=(*IMG_SIZE, 3),
        include_top=False,
        weights="imagenet",
    )
    base.trainable = False

    inputs = keras.Input(shape=(*IMG_SIZE, 3))
    x = base(inputs, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dense(512, activation="relu")(x)
    x = layers.Dropout(0.4)(x)
    x = layers.Dense(256, activation="relu")(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    model = keras.Model(inputs, outputs)
    model.compile(
        optimizer=keras.optimizers.Adam(lr),
        loss="categorical_crossentropy",
        metrics=["accuracy",
                 keras.metrics.TopKCategoricalAccuracy(k=3, name="top3_acc")],
    )
    return model, base

def unfreeze_base(model, base_model, lr=LR_FINETUNE):
    """è§£å‡ base model æœ€å¾Œ N å±¤ç”¨æ–¼ Fine-tuning"""
    base_model.trainable = True
    for layer in base_model.layers[:-UNFREEZE_LAYERS]:
        layer.trainable = False
    trainable = sum(1 for l in base_model.layers if l.trainable)
    print(f"ðŸ”“ Fine-tuningï¼šè§£å‡ {trainable} å±¤ï¼ˆå…± {len(base_model.layers)} å±¤ï¼‰")
    model.compile(
        optimizer=keras.optimizers.Adam(lr),
        loss="categorical_crossentropy",
        metrics=["accuracy",
                 keras.metrics.TopKCategoricalAccuracy(k=3, name="top3_acc")],
    )

# â”€â”€â”€ ä¸»è¨“ç·´æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train():
    print("=" * 62)
    print("ðŸŒ¿ PhytoScan æ¨¡åž‹è¨“ç·´")
    print(f"   {TOTAL_ROUNDS} è¼ª Ã— {EPOCHS_PER_ROUND} epochs = "
          f"{TOTAL_ROUNDS * EPOCHS_PER_ROUND} total epochs")
    print(f"   æ–·é»žçºŒè¨“ï¼š{PROGRESS_FILE}")
    print("=" * 62)

    prog = load_progress()
    start_round = prog["completed_rounds"] + 1

    if start_round > TOTAL_ROUNDS:
        print("ðŸŽ‰ è¨“ç·´å·²å…¨éƒ¨å®Œæˆï¼")
        return

    if start_round > 1:
        print(f"ðŸ”„ åµæ¸¬åˆ°é€²åº¦æª”ï¼Œå¾žç¬¬ {start_round} è¼ªç¹¼çºŒ")

    # è¼‰å…¥è³‡æ–™é›†
    train_ds, val_ds, class_names, num_classes = build_datasets()

    # å»ºç«‹æˆ–è¼‰å…¥æ¨¡åž‹
    prev_ckpt = CKPT_DIR / f"round_{start_round - 1}.keras"
    if start_round > 1 and prev_ckpt.exists():
        print(f"ðŸ“‚ è¼‰å…¥ä¸Šè¼ªæ¨¡åž‹ï¼š{prev_ckpt}")
        model = keras.models.load_model(prev_ckpt)
        base_model = None   # å·²èžåˆï¼ŒFine-tuning éœ€é‡æ–°å–å¾—
    else:
        model, base_model = build_model(num_classes)
        print(f"ðŸ†• å»ºç«‹æ–°æ¨¡åž‹ï¼ˆé¡žåˆ¥ï¼š{num_classes}ï¼‰")

    # â”€â”€ é€è¼ªè¨“ç·´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for rnd in range(start_round, TOTAL_ROUNDS + 1):
        epoch_start = (rnd - 1) * EPOCHS_PER_ROUND + 1
        epoch_end   =  rnd      * EPOCHS_PER_ROUND

        print(f"\n{'â”€' * 62}")
        print(f"  ç¬¬ {rnd:>2}/{TOTAL_ROUNDS} è¼ª  â”‚  Epoch {epoch_start}â€“{epoch_end}")
        print(f"{'â”€' * 62}")

        # Fine-tuning åˆ‡æ›
        if rnd == FINETUNE_START:
            if base_model is None:
                # å¾žå·²å„²å­˜æ¨¡åž‹é‡å»ºæ™‚éœ€è¦å–å›ž base_model
                for layer in model.layers:
                    if "mobilenetv2" in layer.name:
                        base_model = layer
                        break
            if base_model:
                unfreeze_base(model, base_model)
            else:
                print("âš ï¸  ç„¡æ³•å–å¾— base_modelï¼Œè·³éŽè§£å‡")

        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor="val_accuracy", patience=3,
                restore_best_weights=True, verbose=1
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor="val_loss", factor=0.5, patience=2,
                min_lr=1e-7, verbose=1
            ),
            keras.callbacks.ModelCheckpoint(
                filepath=str(CKPT_DIR / f"round_{rnd}_best.keras"),
                monitor="val_accuracy", save_best_only=True, verbose=0
            ),
        ]

        t0 = time.time()
        history = model.fit(
            train_ds,
            epochs=EPOCHS_PER_ROUND,
            validation_data=val_ds,
            callbacks=callbacks,
            verbose=1,
        )
        elapsed = time.time() - t0

        best_val_acc  = max(history.history.get("val_accuracy", [0]))
        best_val_loss = min(history.history.get("val_loss",     [999]))

        # å„²å­˜æœ¬è¼ªå®Œæ•´æ¨¡åž‹ï¼ˆä¾›ä¸‹è¼ªè¼‰å…¥ï¼‰
        ckpt_path = CKPT_DIR / f"round_{rnd}.keras"
        model.save(ckpt_path)

        # æ›´æ–°é€²åº¦
        if best_val_acc > prog["best_val_acc"]:
            prog["best_val_acc"] = float(best_val_acc)
            # åŒæ™‚æ›´æ–°æœ€ä½³æ¨¡åž‹
            best_path = MODEL_DIR / "best_model.keras"
            model.save(best_path)
            print(f"ðŸ† æ–°æœ€ä½³æ¨¡åž‹ï¼val_acc = {best_val_acc:.4f}")

        prog["completed_rounds"] = rnd
        prog["history"].append({
            "round": rnd,
            "val_accuracy":  float(best_val_acc),
            "val_loss":      float(best_val_loss),
            "elapsed_sec":   round(elapsed, 1),
        })
        save_progress(prog)
        print(f"\n  âœ… ç¬¬ {rnd} è¼ªå®Œæˆ  val_acc={best_val_acc:.4f}  "
              f"è€—æ™‚={elapsed:.0f}s  å·²å­˜ï¼š{ckpt_path.name}")

    # â”€â”€ æœ€çµ‚æ¨¡åž‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    final_path = MODEL_DIR / "plant_disease_model.keras"
    model.save(final_path)
    print("\n" + "=" * 62)
    print(f"ðŸŽ‰ è¨“ç·´å…¨éƒ¨å®Œæˆï¼æœ€çµ‚æ¨¡åž‹ï¼š{final_path}")
    print(f"   æœ€ä½³ val_accï¼š{prog['best_val_acc']:.4f}")
    print("\n  è¼ªæ¬¡æ‘˜è¦ï¼š")
    print(f"  {'è¼ª':>4}  {'val_acc':>9}  {'val_loss':>9}  {'è€—æ™‚(s)':>8}")
    print("  " + "-" * 38)
    for h in prog["history"]:
        print(f"  {h['round']:>4}  {h['val_accuracy']:>9.4f}  "
              f"{h['val_loss']:>9.4f}  {h['elapsed_sec']:>8.1f}")
    print("=" * 62)


if __name__ == "__main__":
    train()